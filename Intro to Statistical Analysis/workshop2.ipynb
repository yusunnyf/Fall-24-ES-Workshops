{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZEsHr_LoOwN"
   },
   "source": [
    "# Intro to Statistical Analysis with Python\n",
    "\n",
    "**Date**: November 7th, 2024\n",
    "\n",
    "**Author**: Sunny Fang, yf2610\n",
    "\n",
    "_Created as part of the Barnard College Computing Fellows Program, Fall 2024_\n",
    "\n",
    "\n",
    "By the end of this workshop, students should be able to...\n",
    "1. Write well-documented, interpretable code for statistical testing,\n",
    "2. Interpret and explain statistical test results to an audience,\n",
    "3. Feel comfortable selecting and implementing the appropriate statistical test for their project,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIJhBHyboOwO"
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q33BgRYYoOwO"
   },
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-IvxiF7oOwP"
   },
   "outputs": [],
   "source": [
    "# to process data\n",
    "import pandas as pd\n",
    "\n",
    "# for numerical processing\n",
    "import numpy as np\n",
    "\n",
    "# to plot data\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to carry out statistical testing\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# for datetime processing\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFZRp55doOwP"
   },
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_QdYR7AoOwP"
   },
   "source": [
    "Today, we are using an air pollution dataset from [Kaggle](https://www.kaggle.com/datasets/sogun3/uspollution/data).\n",
    "\n",
    "You can read the file with the following link: `https://drive.google.com/uc?id=1FNWe_pjSONfixgQHPz6o28tpg34Hiwvj`\n",
    "\n",
    "**IMPORTANT NOTE:** usually files end with .csv, but since we are reading a file from a Google Drive link, it looks a bit different here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwH-28JwoOwP"
   },
   "outputs": [],
   "source": [
    "# TODO: replace the filename\n",
    "# read the dataset using the link provided above\n",
    "df = pd.read_csv(\"_____\")\n",
    "\n",
    "# remove max column restriction\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# display the first 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3HyzEDUoOwP"
   },
   "outputs": [],
   "source": [
    "df_cols = df.columns.to_list()\n",
    "print(df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5KcjyGuoOwP"
   },
   "outputs": [],
   "source": [
    "# Carry out basic data preprocessing techniques\n",
    "# step 1: drop NaNs\n",
    "df = df.dropna()\n",
    "\n",
    "# step 2: drop columns we don't need\n",
    "# TODO: keep everything *except for* the first 6 columns\n",
    "# i.e., we don't need: Unnamed: 0.1, Unnamed: 0, State Code,County Code, Site Num, and Address\n",
    "# there are several right answers to this!\n",
    "df = ____\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE0J31ywoOwP"
   },
   "outputs": [],
   "source": [
    "# an important addition: as a good practice, when dealing with datetime objects in a dataframe,\n",
    "df['Date Local'] = pd.to_datetime(df['Date Local'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkAHa8lnoOwP"
   },
   "source": [
    "# 2. T-test example: one-sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2tDj7x4oOwP"
   },
   "source": [
    "On October 1, 2015, the Environmental Protection Agency strengthened the National Ambient Air Quality Standards: \"areas will meet the standards if the 4th highest daily maximum 8-hour ozone concentration per year, averaged over three years, is **equal to or less than 70 ppb** (or 0.07 ppm) ([source](https://19january2017snapshot.epa.gov/ozone-pollution/2015-revision-2008-ozone-national-ambient-air-quality-standards-naaqs-supporting_.html)).\n",
    "\n",
    "Here, we want to test whether or not the state of California (CA) meets the standard. For the purpose of this demonstration, we are going to loosen the \"4th highest daily maximum 8-hour ozone concentration per year\" assumption. Instead, we are going to see if the **average of the max O3 value across three years** meet the standards.\n",
    "\n",
    "**Resources**:\n",
    "- [How to filter by year](https://stackoverflow.com/questions/46878156/pandas-filter-dataframe-rows-with-a-specific-year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: **data slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: let's subset the data together!\n",
    "# step 1: subset the data to get our desired output\n",
    "# note: here, we subset for \"O3 1st Max Value\",\n",
    "# but we can also use \"O3 Mean\" depending on our research question\n",
    "# 1a: subset the dataset, keeping California only, name it 'ca'\n",
    "ca = df[___]\n",
    "\n",
    "# 1b: subset 'ca' to keep data where year is after 2011 (inclusive), name it 'ca_2011'\n",
    "ca_2011 = ca[___]\n",
    "\n",
    "# 1c: subset 'ca_2011' to keep data where year is before 2013 (inclusive), name it 'ca_2011_2013'\n",
    "ca_2011_2013 = ca_2011[___]\n",
    "\n",
    "# 1d: last but not least, we can just keep the columns we want\n",
    "ca_2011_2013 = ca_2011_2013[['Date Local', 'O3 1st Max Value']]\n",
    "\n",
    "# alternatively, we can write all of this in one line!\n",
    "# ca_2011_2013 = df[(df['State'] == 'California') & ((df['Date Local'].dt.year >= 2011) & (df['Date Local'].dt.year <= 2013))][['Date Local','O3 1st Max Value']]\n",
    "\n",
    "# optional but highly recommended: rename columns for easier access\n",
    "ca_2011_2013 = ca_2011_2013.rename(columns={\"Date Local\": \"date\",\n",
    "                                            \"O3 1st Max Value\": \"o3\"})\n",
    "\n",
    "# always a good idea to check the data\n",
    "display(ca_2011_2013.head())\n",
    "\n",
    "# TODO: save file\n",
    "ca_2011_2013.to_csv(\"filename\")\n",
    "# if using Google colab:\n",
    "# from google.colab import files\n",
    "# files.download (\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJglzq5VoOwQ"
   },
   "source": [
    "## step 2: **define hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcfGQHpLoOwQ"
   },
   "source": [
    "Before we jump into data analysis, take a moment and formulate our null and alternative hypotheses:\n",
    "\n",
    "- Null hypothesis ($H_0$): The mean value of the average daily ozone concentration in CA is 0.07.\n",
    "\n",
    "- Alternative Hypothesis ($H_A$): The mean value of the average daily ozone concentration in CA is greater than 0.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LoynAAEoOwQ"
   },
   "source": [
    "## step 3: **define $\\alpha$ (significance level)**\n",
    "\n",
    "Typically, $\\alpha$ is set to be 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick note on null distribution\n",
    "\n",
    "What does the null and alternative hypothesis mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "null_mean = 0.07\n",
    "sample_std = 0.015\n",
    "n = 1000\n",
    "\n",
    "# generate x values for the plot\n",
    "x = np.linspace(0.01, 0.13, 1000)\n",
    "\n",
    "# calculate the normal distribution PDF\n",
    "y = stats.norm.pdf(x, loc=null_mean, scale=sample_std)\n",
    "\n",
    "# declare the figure\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6), dpi = 400)\n",
    "\n",
    "# plot the null hypothesis distribution for all three graphs\n",
    "ax[0].plot(x, y, label='H0 Distribution', color='olive')\n",
    "ax[1].plot(x, y, label='H0 Distribution', color='olive')\n",
    "ax[2].plot(x, y, label='H0 Distribution', color='olive')\n",
    "\n",
    "# === two tailed test ===\n",
    "# i.e., if HA is μ ≠ μ0\n",
    "lower_bound = stats.norm.ppf(0.025, loc=null_mean, scale=sample_std)\n",
    "upper_bound = stats.norm.ppf(0.975, loc=null_mean, scale=sample_std)\n",
    "\n",
    "# add vertical line for null hypothesis mean\n",
    "ax[0].axvline(x=null_mean, color='firebrick', linestyle='--', label='Null Mean (μ_0=0.07)')\n",
    "\n",
    "# fill in the rejection region (alpha = opacity)\n",
    "ax[0].fill_between(x, y, where=(x <= lower_bound) | (x >= upper_bound), color='darkgoldenrod', alpha=0.5)\n",
    "\n",
    "# add annotations for alternative hypothesis regions\n",
    "ax[0].annotate('rejection region', xy=(lower_bound-sample_std*0.7, 0.1*max(y)),\n",
    "             xytext=(lower_bound, 0.25*max(y)),\n",
    "             arrowprops=dict(facecolor='sienna', shrink=0.05),\n",
    "             ha='right')\n",
    "ax[0].annotate('rejection region', xy=(upper_bound+sample_std*0.7, 0.1*max(y)),\n",
    "             xytext=(upper_bound, 0.25*max(y)),\n",
    "             arrowprops=dict(facecolor='sienna', shrink=0.05),\n",
    "             ha='left')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('Two tailed test (HA: μ ≠ μ_0)')\n",
    "\n",
    "# === right tailed test ===\n",
    "# i.e., if HA is μ > μ0\n",
    "right_tail = stats.norm.ppf(0.95, loc=null_mean, scale=sample_std)\n",
    "\n",
    "# add vertical line for null hypothesis mean\n",
    "ax[1].axvline(x=null_mean, color='firebrick', linestyle='--', label='Null Mean (μ_0=0.07)')\n",
    "\n",
    "# fill in the rejection region (alpha = opacity)\n",
    "ax[1].fill_between(x, y, where=(x >= right_tail), color='darkgoldenrod', alpha=0.5)\n",
    "\n",
    "# add annotations for alternative hypothesis regions\n",
    "ax[1].annotate('rejection region', xy=(right_tail+sample_std*0.7, 0.15*max(y)),\n",
    "             xytext=(right_tail, 0.3*max(y)),\n",
    "             arrowprops=dict(facecolor='sienna', shrink=0.05),\n",
    "             ha='left')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].set_title('Right tailed test (HA: μ > μ_0)')\n",
    "\n",
    "# === left tailed test ===\n",
    "# i.e., if HA is μ < μ0\n",
    "left_tail = stats.norm.ppf(0.05, loc=null_mean, scale=sample_std)\n",
    "\n",
    "# add vertical line for null hypothesis mean\n",
    "ax[2].axvline(x=null_mean, color='firebrick', linestyle='--', label='Null Mean (μ_0=0.07)')\n",
    "\n",
    "# fill in the rejection region (alpha = opacity)\n",
    "ax[2].fill_between(x, y, where=(x <= left_tail), color='darkgoldenrod', alpha=0.5)\n",
    "\n",
    "# add annotations for alternative hypothesis regions\n",
    "ax[2].annotate('rejection region', xy=(left_tail-sample_std*0.7, 0.15*max(y)),\n",
    "             xytext=(left_tail, 0.3*max(y)),\n",
    "             arrowprops=dict(facecolor='sienna', shrink=0.05),\n",
    "             ha='right')\n",
    "ax[2].legend(loc='best')\n",
    "ax[2].set_title('Left tailed test (HA: μ < μ_0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYEgDgLooOwQ"
   },
   "source": [
    "## step 4: **check assumptions**\n",
    "\n",
    "Usually in t-tests, there are several assumptions that should be followed:\n",
    "- Independence of samples: samples should be randomly selected\n",
    "- Identically Distributed: samples should come from the same distribution\n",
    "- Normality (or sample size >30): samples should be normally distributed (Q-Q plots)\n",
    "- Equal variances: samples should have equal variances (box plots)\n",
    "\n",
    "In our example, we are conducting a one-sample t-test, so only the first assumption applies. However, due to the nature of our data being collected over time, we are going to loosen the assumption that time has an effect on ground Ozone levels.\n",
    "\n",
    "Resources:\n",
    "- [Tutorial on t-tests](https://www.datacamp.com/tutorial/an-introduction-to-python-t-tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m-ZhATMoOwQ"
   },
   "source": [
    "#### optional step: **data visualization**\n",
    "\n",
    "As seen earlier in the workshop, visualizations can be powerful! Here, we demonstrate some ways we can create visualizations for this question. You can find a section in the end of this note book for skeleton code used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "NqKAUh1IoOwQ",
    "outputId": "e9fd9a38-d807-4115-d1b7-019ecd89471f"
   },
   "outputs": [],
   "source": [
    "# TODO: live coding!\n",
    "# for our question, we can plot...\n",
    "# (1) histogram to see the distribution - directly related to hypothesis testing\n",
    "# (2) scatterplot to see dates with O3 values that exceed the standard\n",
    "\n",
    "# first, declare the figure\n",
    "# syntax: fig, ax = plt.subplots(nrow, ncol, figsize=(width, height), dpi=dots_per_inch)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8), dpi=400)\n",
    "\n",
    "# padding determines the margin between your graphs\n",
    "# think: what would it look like if pad = 1.0? try it yourself!\n",
    "plt.tight_layout(pad=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfeT4aq8GtBU",
    "outputId": "4978af1f-91cf-45a2-e72e-a987e33eb206"
   },
   "outputs": [],
   "source": [
    "# imagine you are \"storing\" your visual in the axes!\n",
    "print(ax)\n",
    "print(len(ax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "HPiVIZ0qAbcr",
    "outputId": "128c5158-550b-449b-f3e0-f2530cb83907"
   },
   "outputs": [],
   "source": [
    "# *full* pipeline\n",
    "# first, declare the figure\n",
    "# syntax: fig, ax = plt.subplots(nrow, ncol, figsize=(width, height), dpi=dots_per_inch)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8), dpi=400)\n",
    "plt.tight_layout(pad=4.0)\n",
    "\n",
    "# next, plot with Seaborn\n",
    "# (1) histogram to see the distribution - directly related to hypothesis testing\n",
    "# (2) scatterplot to see dates with O3 values that exceed the standard\n",
    "# first plot: ax[0] want to see distribution of o3\n",
    "sns.___(x=\"o3\",\n",
    "             data=ca_2011_2013,\n",
    "             label = \"O3 1st Max Value\",\n",
    "             ax=ax[0])\n",
    "\n",
    "# second plot: ax[1] want to dates with O3 values that exceed the standard (0.07)\n",
    "# step 1: plot scatter plot\n",
    "# what should the x and y axis be? remember, we want to see what O3 values look over \"time\"\n",
    "sns.___(x=\"___\",\n",
    "                y=\"___\",\n",
    "                data=ca_2011_2013,\n",
    "                label=\"___\", # what do we want the legend to say?\n",
    "                ax=ax[1])\n",
    "\n",
    "# step 2: we want to \"overlay\" a scatterplot to highlight the points where O3 value > threshold\n",
    "sns.___(x=\"___\",\n",
    "                y=\"___\",\n",
    "                data=ca_2011_2013[ca_2011_2013['o3']>0.07],\n",
    "                label=\"Above threshold\",\n",
    "                color = 'r',\n",
    "                ax=ax[1])\n",
    "\n",
    "# don't forget to set labels and titles\n",
    "ax[0].set_xlabel(\"Ozone 1st max value\")\n",
    "ax[0].set_ylabel(\"Frequency count\")\n",
    "ax[0].set_title(\"Distribution of Ozone in California, 2011-2013\",\n",
    "                fontsize = 16)\n",
    "\n",
    "ax[1].set_xlabel(\"Date\")\n",
    "ax[1].set_ylabel(\"Ozone 1st max value\")\n",
    "ax[1].set_title(\"Ozone Levels in California (2011-2013): Daily Observations with Exceedances Highlighted\",\n",
    "                fontsize = 16)\n",
    "\n",
    "# additional annotations\n",
    "# some variables we might need later\n",
    "threshold = 0.07\n",
    "mean_o3 = ca_2011_2013['o3'].mean()\n",
    "# for the histogram, let's try to annotate the following:\n",
    "# (a) a vertical line to show the EPA threshold\n",
    "ax[0].___(x=0.07, linewidth=3, color='r', label = \"threshold\")\n",
    "\n",
    "# (b) a vertical line to show the mean of the O3\n",
    "ax[0].___(x=ca_2011_2013['o3'].mean(), linewidth=3, color='b', label = \"mean\")\n",
    "\n",
    "# important line, shows the legend\n",
    "ax[0].legend(loc=0, fontsize=14)\n",
    "\n",
    "# for the scatter plot, what do we want to annotate?\n",
    "_____\n",
    "\n",
    "# optional: add caption\n",
    "# caption = \"your caption here\"\n",
    "# fig.text(x, y, caption, ha='center')\n",
    "\n",
    "# TODO: change filename\n",
    "# save figure\n",
    "plt.savefig(\"filename\")\n",
    "# if using Google colab:\n",
    "# from google.colab import files\n",
    "# files.download (\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5: **statistical testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is t-statistic? how is it calculated?\n",
    "\n",
    "$t= \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$, where\n",
    "\n",
    "- $\\bar{x}$ = sample mean of $x$,\n",
    "- $\\mu_0$ = population mean we are testing against\n",
    "- $s$ = sample standard deviation\n",
    "- $\\sqrt{n}$ = square root of the sample size, $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ca_2011_2013['o3']\n",
    "n = len(x)\n",
    "x_bar = np.mean(x)\n",
    "mu_0 = 0.07\n",
    "s = np.std(x, ddof=1) # ddof = 1 for *sample* standard deviation\n",
    "sqrt_n = np.sqrt(n)\n",
    "\n",
    "t_stat = (x_bar - mu_0) / (s / sqrt_n)\n",
    "print(f\"The t-statistic is {t_stat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ca_2011_2013['o3']\n",
    "dof = len(x) - 1\n",
    "x_axis = np.linspace(-5, 5, 100)\n",
    "y = stats.t.pdf(x_axis, dof)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate the critical t-value for the right-tailed test\n",
    "# identical to: critical_value = t.ppf(q = 0.95, df = dof)\n",
    "critical_value = t.ppf(q = 1 - alpha, df = dof)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4), dpi=200)\n",
    "plt.plot(x_axis, y, label='t-distribution', color='olive')\n",
    "\n",
    "# Shade the rejection region\n",
    "x_fill = np.linspace(critical_value, 5, 100)\n",
    "y_fill = t.pdf(x_fill, dof)\n",
    "plt.fill_between(x_fill, y_fill, color='khaki', alpha=0.5, label='Rejection Region')\n",
    "\n",
    "# Add a vertical line at the critical value\n",
    "plt.axvline(x=critical_value, color='darkgoldenrod', linestyle='--', label=f'Critical Value (t={critical_value:.2f})')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('t-Distribution with Rejection Region for Right-Tailed Test')\n",
    "plt.xlabel('t-value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ca_2011_2013['o3']\n",
    "dof = len(x) - 1\n",
    "x_axis = np.linspace(-350, 5, 100)\n",
    "y_axis = np.array([0]*100)\n",
    "\n",
    "x_t = np.linspace(-5, 5, 100)\n",
    "y = stats.t.pdf(x_t, dof)\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate the critical t-value for the right-tailed test\n",
    "# identical to: critical_value = t.ppf(q = 0.95, df = dof)\n",
    "critical_value = stats.t.ppf(q = 1 - alpha, df = dof)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(24, 2), dpi=200)\n",
    "plt.plot(x_axis,y_axis, alpha = 0)\n",
    "plt.plot(x_t, y, label='t-distribution', color='olive')\n",
    "\n",
    "# Shade the rejection region\n",
    "x_fill = np.linspace(critical_value, 5, 100)\n",
    "y_fill = stats.t.pdf(x_fill, dof)\n",
    "plt.fill_between(x_fill, y_fill, color='khaki', alpha=0.5, label='Rejection Region')\n",
    "\n",
    "# Add a vertical line at the critical value\n",
    "plt.axvline(x=critical_value, color='darkgoldenrod', linestyle='--', label=f'Critical Value (t={critical_value:.2f})')\n",
    "plt.axvline(x=t_stat, color='firebrick', linestyle='--', label=f't-statistic ({t_stat:.2f})')\n",
    "# Add labels and title\n",
    "plt.title('t-Distribution with Rejection Region for Right-Tailed Test')\n",
    "plt.xlabel('t-value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some intuition, let's see how we can do this easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conducting t-tests using `scipy.stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peform one-tailed one-sample t-test\n",
    "x = ca_2011_2013['o3']\n",
    "t_stat, p_value = stats.ttest_1samp(a = x,\n",
    "                              popmean = 0.070,\n",
    "                              alternative=\"greater\")\n",
    "print(f\"One-sample t-Test results: \\nt-statistic = {t_stat:0.3f} \\np-value = {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we delete the alternative?\n",
    "t_stat, p_value = stats.ttest_1samp(a = x,\n",
    "                              popmean = 0.070)\n",
    "print(f\"One-sample t-Test results: \\nt-statistic = {t_stat:0.3f} \\np-value = {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we change the alternative to \"less\"?\n",
    "t_stat, p_value = stats.ttest_1samp(a = x,\n",
    "                              popmean = 0.070,\n",
    "                              alternative=\"less\")\n",
    "print(f\"One-sample t-Test results: \\nt-statistic = {t_stat:0.3f} \\np-value = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obWKVzkyoOwQ"
   },
   "source": [
    "## step 6: **reporting your findings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zIbuEhdoOwQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Write your conclusion in the Markdown box below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCD4E7HZoOwQ"
   },
   "source": [
    "(your conclusion here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IMPORTANT NOTE**:\n",
    "There are several variations to the t-test, such as two-sample t-tests (student's t-test and Welch's t-test for unequal variance). The most important skill here for research is:\n",
    "1. Look at your data and formulate a research question.\n",
    "2. Decide the most appropriate statistical test to apply.\n",
    "\n",
    "If you ever have doubts, always ask! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model-fitting example: logistic regression\n",
    "\n",
    "Adopted from [\"Logistic Regression in R - An Example\"](https://www.geo.fu-berlin.de/en/v/soga-r/Basics-of-statistics/Logistic-Regression/Logistic-Regression-in-R---An-Example/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **step 1:** exploratory data analysis\n",
    "\n",
    "AKA read and understand dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurricanes = pd.read_excel(\"https://userpage.fu-berlin.de/soga/data/raw-data/hurricanes.xlsx\")\n",
    "hurricanes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set year to be the index\n",
    "hurricanes.set_index('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the hurricane counts over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts = hurricanes.groupby('Year').count()\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 4), dpi = 400)\n",
    "sns.barplot(x='Year',y='Number',data=yearly_counts, ax = ax,linewidth=0)\n",
    "ax.set_xticks([8*n for n in range(7)])\n",
    "# labels for x and y axis\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Hurricane counts')\n",
    "\n",
    "# title of plot\n",
    "plt.title('Hurricane counts over the years',fontsize=10)\n",
    "ax.set_xticks([8*n for n in range(7)])\n",
    "plt.xticks(rotation=0, ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the hurricane counts over the years, grouped by \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: check how many unique types of hurricane exist\n",
    "# HINT: which column are we using?\n",
    "# HINT: what method do we use to get the count of each category?\n",
    "hurricanes['___'].___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? The hurricane types are labeled by numbers. By looking at the dataset, there's no way for us to understand what Type 0, 1, 3 mean respectively. Therefore, it is always important to read the [metadata](https://myweb.fsu.edu/jelsner/temp/Data.html) and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot another barplot, but this time, we are going to label it by the type of hurricane\n",
    "# but first, let's do some data cleaning\n",
    "yearly_counts = hurricanes.groupby('Year').count()\n",
    "year = list(hurricanes['Year'].unique())\n",
    "y1 = hurricanes[hurricanes['Type']==0].groupby('Year').count()['Number'] # what does Type 0 hurricane correspond to?\n",
    "y2 = hurricanes[hurricanes['Type']==1].groupby('Year').count()['Number'] # what does Type 1 hurricane correspond to?\n",
    "y3 = hurricanes[hurricanes['Type']==3].groupby('Year').count()['Number'] # what does Type 3 hurricane correspond to?\n",
    "hurricanes_by_type = pd.DataFrame({\"tropical-only\": y1, \"baroclinically-enhanced\": y2, \"baroclinically-initiated\": y3}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare digure\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 4), dpi = 400)\n",
    "\n",
    "# create stacked bar chart\n",
    "hurricanes_by_type.plot(kind='bar',stacked=True, color=['red', 'skyblue', 'green'], ax=ax, linewidth=0)\n",
    "\n",
    "# labels for x and y axis\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Hurricane counts')\n",
    "\n",
    "# title of plot\n",
    "plt.title('Hurricane counts over the years',fontsize=10)\n",
    "ax.set_xticks([8*n for n in range(7)])\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.legend(loc=(0.5,0.75),fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last but not least, let's plot the geographical distirbution of the different hurricanes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created with the help of https://www.codeconvert.ai/r-to-python-converter\n",
    "%pip install folium\n",
    "import folium\n",
    "\n",
    "# create map\n",
    "m = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "# add tiles\n",
    "folium.TileLayer('OpenStreetMap').add_to(m)\n",
    "folium.TileLayer('Esri OceanBasemap').add_to(m)\n",
    "\n",
    "# define colors\n",
    "cols = [\"red\", \"navy\", \"__\", \"green\"]\n",
    "\n",
    "# Add circle markers\n",
    "for index, row in hurricanes.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(row['FirstLat'], row['FirstLon']), # latitude, longtitude\n",
    "        radius=2.5,\n",
    "        color=cols[row['Type']], \n",
    "        popup=f\"Year: {row['Year']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                top: 10px; left: 10px; width: 200px; height: 100px; \n",
    "                border:2px solid grey; z-index:9999; font-size:14px; \n",
    "                background-color:white;\">\n",
    "        <h4>Type of Hurricane</h4>\n",
    "        <i style=\"background:red; width:10px; height:10px; display:inline-block;\"></i> tropical<br>\n",
    "        <i style=\"background:navy; width:10px; height:10px; display:inline-block;\"></i> baroclinically-enhanced<br>\n",
    "        <i style=\"background:green; width:10px; height:10px; display:inline-block;\"></i> baroclinically-initiated<br>\n",
    "    </div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **step 2**: define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aside: understanding **logistical regression**\n",
    "Recall how linear regression follows the formula below. It aims to see the predicted value of a dependent variable $y$ given different coefficients.\n",
    "$$y = \\beta_0+\\beta_1x$$\n",
    "\n",
    "Logistical regression also has the parameters $\\beta_i$'s like linera regression. However, the target $y$ is *not* the value of the actual dependent variable but instead what we call \"log-odds.\"\n",
    "\n",
    "In other words, given $\\beta_0$ and $\\beta_1$, you calculate the odds of y=1 using the below equation. \n",
    "\n",
    "$$p(y=1) = \\phi(\\eta) = \\frac{1}{1+e^{-\\eta}}= \\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1)}}$$\n",
    "\n",
    "Some examples:\n",
    "- Deciding the likelihood that today will rain (yes/no)\n",
    "- Predicting whether or not a candidate will get into college\n",
    "- Evaluating the risk of getting breast cancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above map, what do we think is a *potential* factor influencing the type of hurricane (tropical vs. non-tropical) formed?\n",
    "\n",
    "Since latitude affects hurricane type, let's try to fit a logistic regression model for hurrican `Type` and `FirstLat`.\n",
    "\n",
    "To simply the question, let's assume we are doing a **binary classification**: tropical vs. non-tropical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column called new_type \n",
    "# where we encode Type = 0 (tropical) to 0, and Type = 1 or 3 (non-tropical) to 1\n",
    "hurricanes['new_type'] = (hurricanes['Type'] != __).astype(int)\n",
    "\n",
    "# do a values count for sanity check (i.e., we should expect 187 entries for \"0\")\n",
    "hurricanes['new_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **step 3**: fitting the model!\n",
    "\n",
    "To fit models in Python, there are two main libraries: \n",
    "1. `statsmodels.formula.api` that allows \"R-style\" formulas for fitting models (e.g., `smf.ols(formula='y ~ x', data=df))`, see example [here](https://www.statsmodels.org/stable/example_formulas.html) --> designed for *explanatory* purposes\n",
    "\n",
    "2. `scikit-learn` usually designed for *prediction* purposes\n",
    "\n",
    "We will show examples in both to see that they achieve the same results\n",
    "\n",
    "sources:\n",
    "- https://medium.com/@hsrinivasan2/linear-regression-in-scikit-learn-vs-statsmodels-568b60792991\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import necessary packages\n",
    "%pip install scikit-learn\n",
    "%pip install statsmodels\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data: declare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "### what is our X? \n",
    "# in our formula, recall that we have an intercept variable. How do we account that?\n",
    "hurricanes['Intercept'] = 1\n",
    "\n",
    "# HINT: what is the column that represents latitude?\n",
    "X = hurricanes[['__', '__']]  # predictor variables\n",
    "\n",
    "### what is our y?\n",
    "# HINT: what is the variable whose probability we are trying to predict?\n",
    "y = hurricanes['__']  # target variable\n",
    "\n",
    "### print out the shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `statsmodels`\n",
    "\n",
    "1. fit model\n",
    "2. print model summary\n",
    "3. predict on new values\n",
    "\n",
    "source:\n",
    "- https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see all the modles offered by statsmodels, you can print it out!\n",
    "print(dir(sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statsmodels option 1: `sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "\n",
    "# print model summary\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interpreting the results\n",
    "\n",
    "Look at the `coef` column:\n",
    "- Intercept ($\\beta_0$): -9.0826\n",
    "- First Lat ($\\beta_1$): 0.3728\n",
    "\n",
    "Let's write out the by plugging in the $\\beta$ values:\n",
    "$$p(y=1) = \\frac{1}{1+e^{-(-9.0826+0.3728*x_1)}}$$\n",
    "\n",
    "**ACTIVITY:** find the latitude of your home count and plug it in to $x_1$, what is the probability of there is a non-tropical hurricane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting new data\n",
    "# first, let's declare `X_test`\n",
    "X_test = pd.DataFrame({\"Intercept\":[1]*3, \"FirstLat\": [10, 23.5, 30]})\n",
    "\n",
    "# using log_reg\n",
    "predictions = log_reg.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statsmodels option #2: `smf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly different method using smf (good for those who have experience in R!)\n",
    "log_reg_2 = smf.logit(\"new_type ~ FirstLat\", data=hurricanes).fit()\n",
    "\n",
    "# print model summary\n",
    "print(log_reg_2.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source:\n",
    "- https://stackoverflow.com/questions/13218461/predicting-values-using-an-ols-model-with-statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting new data\n",
    "# using log_reg_2 \n",
    "# should yield the same results\n",
    "predictions = log_reg_2.get_prediction(exog = X_test) \n",
    "print(predictions.summary_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary\n",
    "logr_summary = pd.DataFrame(zip(X.columns, [logr.intercept_[0], logr.coef_[0][1]]))\n",
    "logr_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have new data X_new\n",
    "predictions = logr.predict_proba(X_test)\n",
    "print(predictions[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(optional) step 4**: let's visualize what this looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes = np.linspace(min(hurricanes['FirstLat']), max(hurricanes['FirstLat']), 1000)\n",
    "X_new = pd.DataFrame({\"Intercept\":[1]*1000, \"FirstLat\": latitudes})\n",
    "\n",
    "probs = log_reg_2.get_prediction(exog = X_new).summary_frame()\n",
    "\n",
    "pm = probs['predicted']\n",
    "pu = probs['predicted'] + probs['se'] * 1.96  # 95% confidence interval\n",
    "pl = probs['predicted'] - probs['se'] * 1.96  # 95% confidence interval\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4), dpi=400)\n",
    "plt.scatter(hurricanes['FirstLat'], hurricanes['new_type'], s=10, color = \"grey\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Formation Latitude (N)\")\n",
    "\n",
    "plt.grid()\n",
    "plt.fill_between(latitudes, pl, pu, alpha=0.3, color='grey')\n",
    "plt.plot(latitudes, pm, linewidth=2, color = \"olive\")\n",
    "plt.plot(latitudes, pu, linewidth=2, color=\"red\", alpha = 0.3)\n",
    "plt.plot(latitudes, pl, linewidth=2, color=\"red\", alpha = 0.3)\n",
    "\n",
    "plt.axhline(y=0.1, linestyle='--', alpha = 0.3)\n",
    "plt.axhline(y=0.5, linestyle='--', alpha = 0.3)\n",
    "plt.axhline(y=0.9, linestyle='--', alpha = 0.3)\n",
    "\n",
    "plt.annotate('tropical hurricanes', xy=(30, 0.02),\n",
    "             xytext=(35, 0.17),\n",
    "             arrowprops=dict(facecolor='grey', shrink=0.05,linewidth = 0.1),\n",
    "             ha='right')\n",
    "plt.annotate('non-tropical hurricanes', xy=(17, 0.98),\n",
    "             xytext=(10, 0.82*max(y)),\n",
    "             arrowprops=dict(facecolor='grey', shrink=0.05, linewidth = 0.1),\n",
    "             ha='left')\n",
    "\n",
    "plt.annotate('probability of non-tropical hurricanes', xy=(25.5, 0.5),\n",
    "             xytext=(28, 0.35),\n",
    "             arrowprops=dict(facecolor='olive', shrink=0.05, linewidth = 0.1),\n",
    "             ha='left')\n",
    "\n",
    "plt.title(\"Logistic regression for tropical vs. non-tropical hurricanes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pac0GVzKoOwR"
   },
   "source": [
    "# Appendix 1. Visualization Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xq9ny54oOwR"
   },
   "source": [
    "Below is a skeleton code for visualizing with Seaborn and Matplotlib, but this is NOT meant for you to follow the format stringently.\n",
    "```\n",
    "# skeleton code for plotting in python\n",
    "\n",
    "# first, declare the figure\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize = (width, height), dpi = 400)\n",
    "\n",
    "# next, plot with Seaborn\n",
    "# note: change the ax=ax[i] argument if you have multiple subplots\n",
    "sns.histplot(x=iv, data=data, ax=ax)\n",
    "sns.lineplot(x=iv, y=dv, data=data, label=\"legend label\", ax=ax)\n",
    "sns.barplot(x=iv, y=dv, data=data, ax=ax)\n",
    "\n",
    "# don't forget to set labels and titles\n",
    "# similarly, change ax to ax[i] if you have multiple subplots\n",
    "ax.set_xlabel(\"your xlabel here\")\n",
    "ax.set_ylabel(\"your ylabel here\")\n",
    "ax.set_title(\"your title here\")\n",
    "\n",
    "# optional: add caption\n",
    "# similarly, change ax to ax[i] if you have multiple subplots\n",
    "caption = \"your caption here\"\n",
    "ax.text(x, y, caption, ha='center')\n",
    "\n",
    "# save figure\n",
    "plt.savefig(\"filename.png\")\n",
    "# if using Google colab:\n",
    "files.download(\"filename.png\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "barnard-cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
